{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["##### >START FROM HEREEEE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","# add new work arragement column to raw data files\n","import os\n","import pandas as pd\n","\n","# Directory containing all data files and subdirectories\n","root_directory = '/content/drive/MyDrive/Intro_DS/Datasets/RawData/'\n","\n","# File name for file_index CSV\n","file_index_path = '/content/drive/MyDrive/Intro_DS/Datasets/work_argmnt.csv'\n","\n","# Read the file_index CSV\n","file_index_df = pd.read_csv(file_index_path)\n","\n","# Traverse through all files in the directory and subdirectories\n","for subdir, dirs, files in os.walk(root_directory):\n","    for file_name in files:\n","        if file_name.endswith('.csv'):\n","            # Extract the first integer before the first underscore '_'\n","            file_prefix = file_name.split('_')[0]\n","\n","            try:\n","                file_integer = int(file_prefix)\n","            except ValueError:\n","                # If no integer is found, skip the file\n","                continue\n","\n","            # Find the matching row in the file_index file based on file_integer\n","            matching_row = file_index_df[file_index_df['Number'] == file_integer]\n","\n","            if not matching_row.empty:\n","                # Get the corresponding work_arrangement value\n","                work_arrangement_value = matching_row['work_arrangement'].values[0]\n","\n","                # Read the data file\n","                data_file_path = os.path.join(subdir, file_name)\n","                data_df = pd.read_csv(data_file_path)\n","\n","                # Add the new column 'work_arrangement' and fill it with the same value\n","                data_df['work_arrangement'] = work_arrangement_value\n","\n","                # Save the updated DataFrame back to the file (or a new file if needed)\n","                data_df.to_csv(data_file_path, index=False)\n","                print(f\"Updated {file_name} in {subdir} with work arrangement: {work_arrangement_value}\")\n","            else:\n","                print(f\"No matching file_index for {file_name} in {subdir}\")\n"],"metadata":{"id":"P-k_dkUJJtYy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728815443848,"user_tz":-180,"elapsed":28253,"user":{"displayName":"Dilusha Senarathna","userId":"00628219531426179374"}},"outputId":"2a9c0218-fc6b-4763-a2e4-a6b97b3906a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated 1_dataset_linkedin-jobs-scraper_2024-09-12_10-16-49-660.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 34_dataset_linkedin-jobs-scraper_2024-09-12_11-07-48-559.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Remote\n","Updated 3_dataset_linkedin-jobs-scraper_2024-09-12_10-21-52-488.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 63_dataset_linkedin-jobs-scraper_2024-09-12_11-42-01-980.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 2_dataset_linkedin-jobs-scraper_2024-09-12_10-20-19-485.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 22_dataset_linkedin-jobs-scraper_2024-09-12_10-51-05-704.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 4_dataset_linkedin-jobs-scraper_2024-09-12_10-24-38-930.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 14_dataset_linkedin-jobs-scraper_2024-09-12_10-41-42-437.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 32_dataset_linkedin-jobs-scraper_2024-09-12_11-03-56-914.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Remote\n","Updated 64_dataset_linkedin-jobs-scraper_2024-09-12_11-45-30-823.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 11_dataset_linkedin-jobs-scraper_2024-09-12_10-34-38-479.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 31_dataset_linkedin-jobs-scraper_2024-09-12_11-00-45-026.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Remote\n","Updated 42_dataset_linkedin-jobs-scraper_2024-09-12_11-22-15-410.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Remote\n","Updated 62_dataset_linkedin-jobs-scraper_2024-09-12_11-40-49-993.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 83_dataset_linkedin-jobs-scraper_2024-09-12_12-09-42-930.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 71_dataset_linkedin-jobs-scraper_2024-09-12_11-57-05-742.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 72_dataset_linkedin-jobs-scraper_2024-09-12_11-59-14-251.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 74_dataset_linkedin-jobs-scraper_2024-09-12_12-01-43-367.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 65_dataset_linkedin-jobs-scraper_2024-09-12_11-46-44-019.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 84_dataset_linkedin-jobs-scraper_2024-09-12_12-10-36-909.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 1_dataset_linkedin-jobs-scraper_2024-10-11_09-26-38-544.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 2_dataset_linkedin-jobs-scraper_2024-10-11_09-21-49-229.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 5_dataset_linkedin-jobs-scraper_2024-10-11_09-48-51-270.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 4_dataset_linkedin-jobs-scraper_2024-10-11_09-43-38-676.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 7_dataset_linkedin-jobs-scraper_2024-10-11_09-52-12-957.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 10_dataset_linkedin-jobs-scraper_2024-10-11_09-56-29-221.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 11_dataset_linkedin-jobs-scraper_2024-10-11_10-01-54-515.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 21_dataset_linkedin-jobs-scraper_2024-10-11_10-17-30-681.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 22_dataset_linkedin-jobs-scraper_2024-10-11_10-19-21-233.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 23_dataset_linkedin-jobs-scraper_2024-10-11_10-22-00-981.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: On-site\n","Updated 32_dataset_linkedin-jobs-scraper_2024-10-11_10-33-51-668.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Remote\n","Updated 42_dataset_linkedin-jobs-scraper_2024-10-11_10-53-31-030.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Remote\n","Updated 34_dataset_linkedin-jobs-scraper_2024-10-11_10-37-26-961.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Remote\n","Updated 43_dataset_linkedin-jobs-scraper_2024-10-11_10-56-03-435.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Remote\n","Updated 62_dataset_linkedin-jobs-scraper_2024-10-11_11-16-29-270.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 63_dataset_linkedin-jobs-scraper_2024-10-11_11-18-24-478.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 64_dataset_linkedin-jobs-scraper_2024-10-11_11-22-55-529.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 65_dataset_linkedin-jobs-scraper_2024-10-11_11-24-31-054.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 72_dataset_linkedin-jobs-scraper_2024-10-11_11-32-15-669.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 76_dataset_linkedin-jobs-scraper_2024-10-11_11-37-14-838.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 79_dataset_linkedin-jobs-scraper_2024-10-11_11-47-35-133.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 74_dataset_linkedin-jobs-scraper_2024-10-11_11-34-59-921.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 81_dataset_linkedin-jobs-scraper_2024-10-11_11-49-48-780.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist with work arrangement: Hybrid\n","Updated 94_dataset_linkedin-jobs-scraper_2024-09-12_12-24-16-811.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 121_dataset_linkedin-jobs-scraper_2024-09-12_12-55-08-392.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Remote\n","Updated 104_dataset_linkedin-jobs-scraper_2024-09-12_12-35-52-515.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 122_dataset_linkedin-jobs-scraper_2024-09-12_12-58-55-894.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Remote\n","Updated 93_dataset_linkedin-jobs-scraper_2024-09-12_12-22-16-612.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 124_dataset_linkedin-jobs-scraper_2024-09-12_13-02-06-192.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Remote\n","Updated 112_dataset_linkedin-jobs-scraper_2024-09-12_12-44-13-244.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 92_dataset_linkedin-jobs-scraper_2024-09-12_12-20-14-710.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 132_dataset_linkedin-jobs-scraper_2024-09-12_13-12-28-397.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Remote\n","Updated 155_dataset_linkedin-jobs-scraper_2024-09-12_13-36-21-416.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 154_dataset_linkedin-jobs-scraper_2024-09-12_13-34-23-848.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 153_dataset_linkedin-jobs-scraper_2024-09-12_13-32-21-430.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 152_dataset_linkedin-jobs-scraper_2024-09-12_13-29-42-478.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 174_dataset_linkedin-jobs-scraper_2024-09-12_18-21-04-267.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 162_dataset_linkedin-jobs-scraper_2024-09-12_18-05-09-780.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 161_dataset_linkedin-jobs-scraper_2024-09-12_18-03-27-415.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 173_dataset_linkedin-jobs-scraper_2024-09-12_18-18-01-692 (1).csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 164_dataset_linkedin-jobs-scraper_2024-09-12_18-07-05-519.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 91_dataset_linkedin-jobs-scraper_2024-10-11_12-07-33-788.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 92_dataset_linkedin-jobs-scraper_2024-10-11_12-09-48-804.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 93_dataset_linkedin-jobs-scraper_2024-10-11_12-11-53-864.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 94_dataset_linkedin-jobs-scraper_2024-10-11_12-14-57-618.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 109_dataset_linkedin-jobs-scraper_2024-10-11_15-05-48-463.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 97_dataset_linkedin-jobs-scraper_2024-10-11_14-51-40-083.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 111_dataset_linkedin-jobs-scraper_2024-10-11_15-09-57-842.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 112_dataset_linkedin-jobs-scraper_2024-10-11_15-17-38-719.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: On-site\n","Updated 122_dataset_linkedin-jobs-scraper_2024-10-11_15-30-58-603.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Remote\n","Updated 132_dataset_linkedin-jobs-scraper_2024-10-11_15-49-25-258.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Remote\n","Updated 124_dataset_linkedin-jobs-scraper_2024-10-11_15-34-24-820.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Remote\n","Updated 133_dataset_linkedin-jobs-scraper_2024-10-11_15-51-20-838.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Remote\n","Updated 152__dataset_linkedin-jobs-scraper_2024-10-11_16-40-29-367.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 154_dataset_linkedin-jobs-scraper_2024-10-11_16-33-56-548.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 153_dataset_linkedin-jobs-scraper_2024-10-11_16-43-30-557.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 162_dataset_linkedin-jobs-scraper_2024-10-11_16-53-28-393.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 155_dataset_linkedin-jobs-scraper_2024-10-11_16-45-05-300.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 169_dataset_linkedin-jobs-scraper_2024-10-11_17-01-47-352.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 164_dataset_linkedin-jobs-scraper_2024-10-11_16-56-16-870.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n","Updated 171_dataset_linkedin-jobs-scraper_2024-10-11_17-06-30-059.csv in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst with work arrangement: Hybrid\n"]}]},{"cell_type":"code","source":["# Main Class\n","\n","def main():\n","  #STEP 1 :\n","  #READ RAW FILES ####\n","    # Define the paths to the folders containing the CSV files\n","    folder_paths = [\n","    '/content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst',\n","    '/content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist'  # Replace with the actual path of the second folder\n","]\n","\n","    # Create an instance of the DataPreprocessing class\n","   # data_preprocessor = DataPreprocessing(folder_paths)\n","\n","    # Call the read_files method to combine CSV files\n","    combined_df = read_files(folder_paths)\n","\n","    # Save the combined DataFrame to a new CSV file if it's not empty\n","    if not combined_df.empty:\n","        combined_df.to_csv('/content/drive/MyDrive/Intro_DS/MiniProject/outputs/combined_data_file.csv', index=False)\n","        print(\"All CSV files have been successfully combined!\")\n","    else:\n","        print(\"No data to save.\")\n","\n","    #STEP 2: REMOVE IDENTICAL RAWS\n","    input_path = '/content/drive/MyDrive/Intro_DS/MiniProject/outputs/combined_data_file.csv'\n","    output_path1 = '/content/drive/MyDrive/Intro_DS/MiniProject/outputs/combined_cleaned_1.csv'\n","    cleaned_df= remove_duplicates(input_path, output_path1)\n","    print(\"REMOVED IDENTICAL RAWS!!!\")\n","    # STEP 3: LANGUAGE TRASLATION IF NEEDED\n","    # Specify the CSV file name here\n","    # Call the language detection function\n","    updated_csv_file = add_language_column_and_filter(output_path1)\n","    without_finnish_csv = '/content/drive/MyDrive/Intro_DS/MiniProject/outputs/combined_cleaned_1_without_finnish.csv'\n","    translated_finnish_csv = '/content/drive/MyDrive/Intro_DS/MiniProject/outputs/combined_cleaned_1_finnish_translated.csv'\n","    combined_csv_file = combine_csv_files(without_finnish_csv, translated_finnish_csv)\n","    print(\"TRANSLATED TO ENGLISH!!!\")\n","\n","    # STEP 4: FEATURE ENGINEERING\n","\n","    cleaned_df='/content/drive/MyDrive/Intro_DS/MiniProject/outputs/combined_csv_file.csv'\n","    cleaned_df = pd.read_csv(cleaned_df)\n","    #output_file_path2 = '/content/drive/MyDrive/Intro_DS/MiniProject/outputs/combined_with_data_science_skills.csv'\n","    keywords_file_path = '/content/drive/MyDrive/Intro_DS/Datasets/keywords.txt'\n","    # Load the keywords from the text file\n","    keywords = read_keywords_from_file(keywords_file_path)\n","    cleaned_df[['required skills', 'Data Base Applications', 'Data Engineering', 'Cloud Computing', 'Soft Skills']] = cleaned_df['description'].apply(lambda desc: pd.Series(extract_info(desc, keywords)))\n","\n","    print(\"processed _job_descriptions!!!\")\n","    print(cleaned_df)\n","    df = cleaned_df\n","\n","    # STEP 5: JPB APPLICANTS COLUMN PROCESSING\n","    # Remove the word 'applicants' from the strings in the column\n","    df['applicationsCount'] = df['applicationsCount'].str.replace(' applicants', '', regex=False)\n","    # Apply the function to the column\n","    df['applicationsCount'] = df['applicationsCount'].apply(replace_qualitative_value)\n","    print(df['applicationsCount'])\n","    df= categorize_into_intervals(df,'applicationsCount' )\n","    print(df['applicationsCount_interval'])\n","    print(\"Processed applicationsCount and created new column applicationsCount_interval\")\n","\n","    # classify the company level using job description column\n","\n","    df['company_level'] = df['description'].apply(lambda x: classify_company_level(x, keywords))\n","    print(df['company_level'])\n","    print(\"Processed description and created new column company_level\")\n","\n","    #Location column splite\n","    # Correct usage of str.split() to split the 'location' column into two new columns\n","    df[['location_1', 'location_2']] = df['location'].str.split(',', n=1, expand=True)\n","\n","    # Strip any leading or trailing whitespace\n","    df['location_1'] = df['location_1'].str.strip()\n","    df['location_2'] = df['location_2'].str.strip()\n","\n","    #### processing postedTime column\n","    # Apply the function to get the number of weeks\n","    df['displayed_weeks'] = df['postedTime'].apply(convert_to_weeks)\n","    # Apply the categorization function to get the job_displayedTime\n","    df['job_displayedTime'] = df['displayed_weeks'].apply(categorize_weeks)\n","    print(\"Processed postedTime and created new column job_displayedTime\")\n","\n","    # Convert 'publishedAt' to datetime format\n","    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n","    # Extract the month as a number from the 'publishedAt' column\n","    df['published_month'] = df['publishedAt'].dt.strftime('%m')  # Month as number\n","    print(\"Processed publishedAt and created new column published_month\")\n","\n","    ### derive type of work with job title\n","    # Apply the function to the DataFrame\n","    df[['general_job_category', 'job_title_desc']] = df['title'].apply(lambda x: pd.Series(extract_tokens_and_text(x)))\n","\n","    # Split the text in 'job_title_desc' into a list of words\n","    df['job_title_desc_list'] = df['job_title_desc'].str.split()\n","    print(df['job_title_desc_list'])\n","    print(\"Processed title and created new column job_title_desc_list\")\n","\n","\n","    ### process worktype column\n","    # Apply the function to split and filter text\n","    df['worktype_desc'] = df['workType'].apply(split_and_filter)\n","    print(df['worktype_desc'])\n","    print(\"Processed workType and created new column worktype_desc\")\n","\n","    ################################ DATA ENGINEERING END  ############################\n","\n","    ################################## DATA PROFILING PART ############################\n","    result = data_profiling(df)\n","    print(result)\n","\n","    # Add more to data profiling functions if need to data cleaning\n","    ################################## END OF DATA PROFILING! ########################\n","    main_save_path = '/content/drive/MyDrive/Intro_DS/MiniProject/outputs/'\n","    cleaned_df = data_profiling(df, duplicates_path='duplicates.csv', summary_path= main_save_path+'data_prifile_before_cleaning.txt')\n","    cleaned_df.to_csv(main_save_path+'cleaned_dataset_v1.csv', index=False)  # Save the cleaned DataFrame FULL\n","\n","    # Create DataFrame without the description column\n","    df_no_description = cleaned_df.drop(columns=['description'])\n","\n","    # Save the DataFrame without the description column\n","    df_no_description.to_csv(main_save_path+'df_no_description.csv', index=False)\n","\n","    # Create DataFrame with specified columns\n","    desc_df = df[['applyUrl', 'companyId', 'companyName', 'description']]\n","\n","    # Save the DataFrame with specified columns\n","    desc_df.to_csv(main_save_path+'desc_df.csv', index=False)\n","\n","    # Columns to check for null counts\n","    columns_to_check = ['required skills', 'Data Base Applications', 'Data Engineering', 'Cloud Computing', 'Soft Skills']\n","\n","    # Calculate and print null counts for each column\n","    null_counts = cleaned_df[columns_to_check].isnull().sum()\n","\n","    # Display null counts\n","    print(null_counts)\n","\n","    ################################## Unknown value count ########################\n","    file_path = '/content/drive/MyDrive/Intro_DS/MiniProject/outputs/cleaned_dataset_v1.csv'\n","    columns_to_check = ['required skills', 'Data Base Applications', 'Data Engineering', 'Cloud Computing', 'Soft Skills']\n","    unknown_counts = count_unknown_values_in_csv(file_path, columns_to_check)\n","    print(unknown_counts)\n","\n"],"metadata":{"id":"Ax19ONA89X8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzETNeritJdl","executionInfo":{"status":"ok","timestamp":1728816017406,"user_tz":-180,"elapsed":27336,"user":{"displayName":"Dilusha Senarathna","userId":"00628219531426179374"}},"outputId":"7d9bc79f-aca6-49ff-e2c7-41a1d9b18f5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CSV files found in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Analyst: ['94_dataset_linkedin-jobs-scraper_2024-09-12_12-24-16-811.csv', '121_dataset_linkedin-jobs-scraper_2024-09-12_12-55-08-392.csv', '104_dataset_linkedin-jobs-scraper_2024-09-12_12-35-52-515.csv', '122_dataset_linkedin-jobs-scraper_2024-09-12_12-58-55-894.csv', '93_dataset_linkedin-jobs-scraper_2024-09-12_12-22-16-612.csv', '124_dataset_linkedin-jobs-scraper_2024-09-12_13-02-06-192.csv', '112_dataset_linkedin-jobs-scraper_2024-09-12_12-44-13-244.csv', '92_dataset_linkedin-jobs-scraper_2024-09-12_12-20-14-710.csv', '132_dataset_linkedin-jobs-scraper_2024-09-12_13-12-28-397.csv', '155_dataset_linkedin-jobs-scraper_2024-09-12_13-36-21-416.csv', '154_dataset_linkedin-jobs-scraper_2024-09-12_13-34-23-848.csv', '153_dataset_linkedin-jobs-scraper_2024-09-12_13-32-21-430.csv', '152_dataset_linkedin-jobs-scraper_2024-09-12_13-29-42-478.csv', '174_dataset_linkedin-jobs-scraper_2024-09-12_18-21-04-267.csv', '162_dataset_linkedin-jobs-scraper_2024-09-12_18-05-09-780.csv', '161_dataset_linkedin-jobs-scraper_2024-09-12_18-03-27-415.csv', '173_dataset_linkedin-jobs-scraper_2024-09-12_18-18-01-692 (1).csv', '164_dataset_linkedin-jobs-scraper_2024-09-12_18-07-05-519.csv', '91_dataset_linkedin-jobs-scraper_2024-10-11_12-07-33-788.csv', '92_dataset_linkedin-jobs-scraper_2024-10-11_12-09-48-804.csv', '93_dataset_linkedin-jobs-scraper_2024-10-11_12-11-53-864.csv', '94_dataset_linkedin-jobs-scraper_2024-10-11_12-14-57-618.csv', '109_dataset_linkedin-jobs-scraper_2024-10-11_15-05-48-463.csv', '97_dataset_linkedin-jobs-scraper_2024-10-11_14-51-40-083.csv', '111_dataset_linkedin-jobs-scraper_2024-10-11_15-09-57-842.csv', '112_dataset_linkedin-jobs-scraper_2024-10-11_15-17-38-719.csv', '122_dataset_linkedin-jobs-scraper_2024-10-11_15-30-58-603.csv', '132_dataset_linkedin-jobs-scraper_2024-10-11_15-49-25-258.csv', '124_dataset_linkedin-jobs-scraper_2024-10-11_15-34-24-820.csv', '133_dataset_linkedin-jobs-scraper_2024-10-11_15-51-20-838.csv', '152__dataset_linkedin-jobs-scraper_2024-10-11_16-40-29-367.csv', '154_dataset_linkedin-jobs-scraper_2024-10-11_16-33-56-548.csv', '153_dataset_linkedin-jobs-scraper_2024-10-11_16-43-30-557.csv', '162_dataset_linkedin-jobs-scraper_2024-10-11_16-53-28-393.csv', '155_dataset_linkedin-jobs-scraper_2024-10-11_16-45-05-300.csv', '169_dataset_linkedin-jobs-scraper_2024-10-11_17-01-47-352.csv', '164_dataset_linkedin-jobs-scraper_2024-10-11_16-56-16-870.csv', '171_dataset_linkedin-jobs-scraper_2024-10-11_17-06-30-059.csv']\n","Loaded DataFrame from 94_dataset_linkedin-jobs-scraper_2024-09-12_12-24-16-811.csv: 50 rows\n","Loaded DataFrame from 121_dataset_linkedin-jobs-scraper_2024-09-12_12-55-08-392.csv: 1 rows\n","Loaded DataFrame from 104_dataset_linkedin-jobs-scraper_2024-09-12_12-35-52-515.csv: 2 rows\n","Loaded DataFrame from 122_dataset_linkedin-jobs-scraper_2024-09-12_12-58-55-894.csv: 30 rows\n","Loaded DataFrame from 93_dataset_linkedin-jobs-scraper_2024-09-12_12-22-16-612.csv: 6 rows\n","Loaded DataFrame from 124_dataset_linkedin-jobs-scraper_2024-09-12_13-02-06-192.csv: 35 rows\n","Loaded DataFrame from 112_dataset_linkedin-jobs-scraper_2024-09-12_12-44-13-244.csv: 1 rows\n","Loaded DataFrame from 92_dataset_linkedin-jobs-scraper_2024-09-12_12-20-14-710.csv: 14 rows\n","Loaded DataFrame from 132_dataset_linkedin-jobs-scraper_2024-09-12_13-12-28-397.csv: 1 rows\n","Loaded DataFrame from 155_dataset_linkedin-jobs-scraper_2024-09-12_13-36-21-416.csv: 1 rows\n","Loaded DataFrame from 154_dataset_linkedin-jobs-scraper_2024-09-12_13-34-23-848.csv: 49 rows\n","Loaded DataFrame from 153_dataset_linkedin-jobs-scraper_2024-09-12_13-32-21-430.csv: 8 rows\n","Loaded DataFrame from 152_dataset_linkedin-jobs-scraper_2024-09-12_13-29-42-478.csv: 21 rows\n","Loaded DataFrame from 174_dataset_linkedin-jobs-scraper_2024-09-12_18-21-04-267.csv: 1 rows\n","Loaded DataFrame from 162_dataset_linkedin-jobs-scraper_2024-09-12_18-05-09-780.csv: 1 rows\n","Loaded DataFrame from 161_dataset_linkedin-jobs-scraper_2024-09-12_18-03-27-415.csv: 1 rows\n","Loaded DataFrame from 173_dataset_linkedin-jobs-scraper_2024-09-12_18-18-01-692 (1).csv: 1 rows\n","Loaded DataFrame from 164_dataset_linkedin-jobs-scraper_2024-09-12_18-07-05-519.csv: 2 rows\n","Loaded DataFrame from 91_dataset_linkedin-jobs-scraper_2024-10-11_12-07-33-788.csv: 1 rows\n","Loaded DataFrame from 92_dataset_linkedin-jobs-scraper_2024-10-11_12-09-48-804.csv: 10 rows\n","Loaded DataFrame from 93_dataset_linkedin-jobs-scraper_2024-10-11_12-11-53-864.csv: 3 rows\n","Loaded DataFrame from 94_dataset_linkedin-jobs-scraper_2024-10-11_12-14-57-618.csv: 48 rows\n","Loaded DataFrame from 109_dataset_linkedin-jobs-scraper_2024-10-11_15-05-48-463.csv: 1 rows\n","Loaded DataFrame from 97_dataset_linkedin-jobs-scraper_2024-10-11_14-51-40-083.csv: 1 rows\n","Loaded DataFrame from 111_dataset_linkedin-jobs-scraper_2024-10-11_15-09-57-842.csv: 1 rows\n","Loaded DataFrame from 112_dataset_linkedin-jobs-scraper_2024-10-11_15-17-38-719.csv: 1 rows\n","Loaded DataFrame from 122_dataset_linkedin-jobs-scraper_2024-10-11_15-30-58-603.csv: 17 rows\n","Loaded DataFrame from 132_dataset_linkedin-jobs-scraper_2024-10-11_15-49-25-258.csv: 1 rows\n","Loaded DataFrame from 124_dataset_linkedin-jobs-scraper_2024-10-11_15-34-24-820.csv: 33 rows\n","Loaded DataFrame from 133_dataset_linkedin-jobs-scraper_2024-10-11_15-51-20-838.csv: 1 rows\n","Loaded DataFrame from 152__dataset_linkedin-jobs-scraper_2024-10-11_16-40-29-367.csv: 15 rows\n","Loaded DataFrame from 154_dataset_linkedin-jobs-scraper_2024-10-11_16-33-56-548.csv: 96 rows\n","Loaded DataFrame from 153_dataset_linkedin-jobs-scraper_2024-10-11_16-43-30-557.csv: 15 rows\n","Loaded DataFrame from 162_dataset_linkedin-jobs-scraper_2024-10-11_16-53-28-393.csv: 1 rows\n","Loaded DataFrame from 155_dataset_linkedin-jobs-scraper_2024-10-11_16-45-05-300.csv: 3 rows\n","Loaded DataFrame from 169_dataset_linkedin-jobs-scraper_2024-10-11_17-01-47-352.csv: 1 rows\n","Loaded DataFrame from 164_dataset_linkedin-jobs-scraper_2024-10-11_16-56-16-870.csv: 4 rows\n","Loaded DataFrame from 171_dataset_linkedin-jobs-scraper_2024-10-11_17-06-30-059.csv: 1 rows\n","CSV files found in /content/drive/MyDrive/Intro_DS/Datasets/RawData/Data_Scientist: ['1_dataset_linkedin-jobs-scraper_2024-09-12_10-16-49-660.csv', '34_dataset_linkedin-jobs-scraper_2024-09-12_11-07-48-559.csv', '3_dataset_linkedin-jobs-scraper_2024-09-12_10-21-52-488.csv', '63_dataset_linkedin-jobs-scraper_2024-09-12_11-42-01-980.csv', '2_dataset_linkedin-jobs-scraper_2024-09-12_10-20-19-485.csv', '22_dataset_linkedin-jobs-scraper_2024-09-12_10-51-05-704.csv', '4_dataset_linkedin-jobs-scraper_2024-09-12_10-24-38-930.csv', '14_dataset_linkedin-jobs-scraper_2024-09-12_10-41-42-437.csv', '32_dataset_linkedin-jobs-scraper_2024-09-12_11-03-56-914.csv', '64_dataset_linkedin-jobs-scraper_2024-09-12_11-45-30-823.csv', '11_dataset_linkedin-jobs-scraper_2024-09-12_10-34-38-479.csv', '31_dataset_linkedin-jobs-scraper_2024-09-12_11-00-45-026.csv', '42_dataset_linkedin-jobs-scraper_2024-09-12_11-22-15-410.csv', '62_dataset_linkedin-jobs-scraper_2024-09-12_11-40-49-993.csv', '83_dataset_linkedin-jobs-scraper_2024-09-12_12-09-42-930.csv', '71_dataset_linkedin-jobs-scraper_2024-09-12_11-57-05-742.csv', '72_dataset_linkedin-jobs-scraper_2024-09-12_11-59-14-251.csv', '74_dataset_linkedin-jobs-scraper_2024-09-12_12-01-43-367.csv', '65_dataset_linkedin-jobs-scraper_2024-09-12_11-46-44-019.csv', '84_dataset_linkedin-jobs-scraper_2024-09-12_12-10-36-909.csv', '1_dataset_linkedin-jobs-scraper_2024-10-11_09-26-38-544.csv', '2_dataset_linkedin-jobs-scraper_2024-10-11_09-21-49-229.csv', '5_dataset_linkedin-jobs-scraper_2024-10-11_09-48-51-270.csv', '4_dataset_linkedin-jobs-scraper_2024-10-11_09-43-38-676.csv', '7_dataset_linkedin-jobs-scraper_2024-10-11_09-52-12-957.csv', '10_dataset_linkedin-jobs-scraper_2024-10-11_09-56-29-221.csv', '11_dataset_linkedin-jobs-scraper_2024-10-11_10-01-54-515.csv', '21_dataset_linkedin-jobs-scraper_2024-10-11_10-17-30-681.csv', '22_dataset_linkedin-jobs-scraper_2024-10-11_10-19-21-233.csv', '23_dataset_linkedin-jobs-scraper_2024-10-11_10-22-00-981.csv', '32_dataset_linkedin-jobs-scraper_2024-10-11_10-33-51-668.csv', '42_dataset_linkedin-jobs-scraper_2024-10-11_10-53-31-030.csv', '34_dataset_linkedin-jobs-scraper_2024-10-11_10-37-26-961.csv', '43_dataset_linkedin-jobs-scraper_2024-10-11_10-56-03-435.csv', '62_dataset_linkedin-jobs-scraper_2024-10-11_11-16-29-270.csv', '63_dataset_linkedin-jobs-scraper_2024-10-11_11-18-24-478.csv', '64_dataset_linkedin-jobs-scraper_2024-10-11_11-22-55-529.csv', '65_dataset_linkedin-jobs-scraper_2024-10-11_11-24-31-054.csv', '72_dataset_linkedin-jobs-scraper_2024-10-11_11-32-15-669.csv', '76_dataset_linkedin-jobs-scraper_2024-10-11_11-37-14-838.csv', '79_dataset_linkedin-jobs-scraper_2024-10-11_11-47-35-133.csv', '74_dataset_linkedin-jobs-scraper_2024-10-11_11-34-59-921.csv', '81_dataset_linkedin-jobs-scraper_2024-10-11_11-49-48-780.csv']\n","Loaded DataFrame from 1_dataset_linkedin-jobs-scraper_2024-09-12_10-16-49-660.csv: 15 rows\n","Loaded DataFrame from 34_dataset_linkedin-jobs-scraper_2024-09-12_11-07-48-559.csv: 42 rows\n","Loaded DataFrame from 3_dataset_linkedin-jobs-scraper_2024-09-12_10-21-52-488.csv: 5 rows\n","Loaded DataFrame from 63_dataset_linkedin-jobs-scraper_2024-09-12_11-42-01-980.csv: 6 rows\n","Loaded DataFrame from 2_dataset_linkedin-jobs-scraper_2024-09-12_10-20-19-485.csv: 26 rows\n","Loaded DataFrame from 22_dataset_linkedin-jobs-scraper_2024-09-12_10-51-05-704.csv: 1 rows\n","Loaded DataFrame from 4_dataset_linkedin-jobs-scraper_2024-09-12_10-24-38-930.csv: 49 rows\n","Loaded DataFrame from 14_dataset_linkedin-jobs-scraper_2024-09-12_10-41-42-437.csv: 1 rows\n","Loaded DataFrame from 32_dataset_linkedin-jobs-scraper_2024-09-12_11-03-56-914.csv: 20 rows\n","Loaded DataFrame from 64_dataset_linkedin-jobs-scraper_2024-09-12_11-45-30-823.csv: 46 rows\n","Loaded DataFrame from 11_dataset_linkedin-jobs-scraper_2024-09-12_10-34-38-479.csv: 3 rows\n","Loaded DataFrame from 31_dataset_linkedin-jobs-scraper_2024-09-12_11-00-45-026.csv: 1 rows\n","Loaded DataFrame from 42_dataset_linkedin-jobs-scraper_2024-09-12_11-22-15-410.csv: 1 rows\n","Loaded DataFrame from 62_dataset_linkedin-jobs-scraper_2024-09-12_11-40-49-993.csv: 25 rows\n","Loaded DataFrame from 83_dataset_linkedin-jobs-scraper_2024-09-12_12-09-42-930.csv: 1 rows\n","Loaded DataFrame from 71_dataset_linkedin-jobs-scraper_2024-09-12_11-57-05-742.csv: 7 rows\n","Loaded DataFrame from 72_dataset_linkedin-jobs-scraper_2024-09-12_11-59-14-251.csv: 1 rows\n","Loaded DataFrame from 74_dataset_linkedin-jobs-scraper_2024-09-12_12-01-43-367.csv: 2 rows\n","Loaded DataFrame from 65_dataset_linkedin-jobs-scraper_2024-09-12_11-46-44-019.csv: 2 rows\n","Loaded DataFrame from 84_dataset_linkedin-jobs-scraper_2024-09-12_12-10-36-909.csv: 1 rows\n","Loaded DataFrame from 1_dataset_linkedin-jobs-scraper_2024-10-11_09-26-38-544.csv: 30 rows\n","Loaded DataFrame from 2_dataset_linkedin-jobs-scraper_2024-10-11_09-21-49-229.csv: 10 rows\n","Loaded DataFrame from 5_dataset_linkedin-jobs-scraper_2024-10-11_09-48-51-270.csv: 1 rows\n","Loaded DataFrame from 4_dataset_linkedin-jobs-scraper_2024-10-11_09-43-38-676.csv: 2 rows\n","Loaded DataFrame from 7_dataset_linkedin-jobs-scraper_2024-10-11_09-52-12-957.csv: 1 rows\n","Loaded DataFrame from 10_dataset_linkedin-jobs-scraper_2024-10-11_09-56-29-221.csv: 1 rows\n","Loaded DataFrame from 11_dataset_linkedin-jobs-scraper_2024-10-11_10-01-54-515.csv: 5 rows\n","Loaded DataFrame from 21_dataset_linkedin-jobs-scraper_2024-10-11_10-17-30-681.csv: 1 rows\n","Loaded DataFrame from 22_dataset_linkedin-jobs-scraper_2024-10-11_10-19-21-233.csv: 1 rows\n","Loaded DataFrame from 23_dataset_linkedin-jobs-scraper_2024-10-11_10-22-00-981.csv: 1 rows\n","Loaded DataFrame from 32_dataset_linkedin-jobs-scraper_2024-10-11_10-33-51-668.csv: 15 rows\n","Loaded DataFrame from 42_dataset_linkedin-jobs-scraper_2024-10-11_10-53-31-030.csv: 1 rows\n","Loaded DataFrame from 34_dataset_linkedin-jobs-scraper_2024-10-11_10-37-26-961.csv: 39 rows\n","Loaded DataFrame from 43_dataset_linkedin-jobs-scraper_2024-10-11_10-56-03-435.csv: 1 rows\n","Loaded DataFrame from 62_dataset_linkedin-jobs-scraper_2024-10-11_11-16-29-270.csv: 12 rows\n","Loaded DataFrame from 63_dataset_linkedin-jobs-scraper_2024-10-11_11-18-24-478.csv: 14 rows\n","Loaded DataFrame from 64_dataset_linkedin-jobs-scraper_2024-10-11_11-22-55-529.csv: 96 rows\n","Loaded DataFrame from 65_dataset_linkedin-jobs-scraper_2024-10-11_11-24-31-054.csv: 5 rows\n","Loaded DataFrame from 72_dataset_linkedin-jobs-scraper_2024-10-11_11-32-15-669.csv: 1 rows\n","Loaded DataFrame from 76_dataset_linkedin-jobs-scraper_2024-10-11_11-37-14-838.csv: 1 rows\n","Loaded DataFrame from 79_dataset_linkedin-jobs-scraper_2024-10-11_11-47-35-133.csv: 1 rows\n","Loaded DataFrame from 74_dataset_linkedin-jobs-scraper_2024-10-11_11-34-59-921.csv: 4 rows\n","Loaded DataFrame from 81_dataset_linkedin-jobs-scraper_2024-10-11_11-49-48-780.csv: 1 rows\n","All CSV files have been successfully combined!\n","Number of records before removing duplicates: 978\n","Number of records after removing duplicates: 755\n","Number of duplicate records removed: 223\n","REMOVED IDENTICAL RAWS!!!\n","Updated CSV Headings (without Finnish records):\n","['applicationsCount', 'applyType', 'applyUrl', 'companyId', 'companyName', 'companyUrl', 'contractType', 'description', 'experienceLevel', 'jobUrl', 'location', 'postedTime', 'posterFullName', 'posterProfileUrl', 'publishedAt', 'salary', 'sector', 'title', 'workType', 'work_arrangement', 'benefits', 'id', 'language_posted']\n","Number of records in the Finnish records CSV file: 110\n","Number of records in the updated CSV file (without Finnish records): 645\n","Number of records in the Combined CSV file: 755\n","Combined CSV saved as /content/drive/MyDrive/Intro_DS/MiniProject/outputs/combined_csv_file.csv\n","TRANSLATED TO ENGLISH!!!\n","processed _job_descriptions!!!\n","                    applicationsCount applyType  \\\n","0                       57 applicants  EXTERNAL   \n","1                       46 applicants  EXTERNAL   \n","2                       72 applicants  EXTERNAL   \n","3    Be among the first 25 applicants  EXTERNAL   \n","4                       42 applicants  EXTERNAL   \n","..                                ...       ...   \n","750                     29 applicants  EXTERNAL   \n","751  Be among the first 25 applicants  EXTERNAL   \n","752                     25 applicants  EXTERNAL   \n","753  Be among the first 25 applicants  EXTERNAL   \n","754  Be among the first 25 applicants  EXTERNAL   \n","\n","                                              applyUrl   companyId  \\\n","0    https://rekry.biisoni.fi/en-GB/jobs/4901632-se...   2936094.0   \n","1    https://wolt.com/en/jobs/posting/7440000089556...   3995846.0   \n","2    https://starship.teamtailor.com/jobs/4933132-s...   6646555.0   \n","3    https://MatchaTalent.oorwin.com/careers/index....  14655911.0   \n","4    https://wolt.com/en/jobs/posting/7440000128565...   3995846.0   \n","..                                                 ...         ...   \n","750  https://atalent.fi/open-position/diktamen-lead...   2845630.0   \n","751  https://emp.jobylon.com/jobs/253891-k-ryhma-se...      7995.0   \n","752  https://ats.talentadore.com/apply/senior-front...  76114935.0   \n","753  https://ats.talentadore.com/apply/devops-engin...      5119.0   \n","754  https://www.poolia.fi/it-arkkitehteja-ja-fulls...  26504096.0   \n","\n","               companyName                                         companyUrl  \\\n","0             Axopar Boats  https://fi.linkedin.com/company/axopar-boats?t...   \n","1                     Wolt  https://fi.linkedin.com/company/wolt-oy?trk=pu...   \n","2    Starship Technologies  https://www.linkedin.com/company/starshiptechn...   \n","3             MatchaTalent  https://id.linkedin.com/company/matchatalent?t...   \n","4                     Wolt  https://fi.linkedin.com/company/wolt-oy?trk=pu...   \n","..                     ...                                                ...   \n","750              Dictation  https://fi.linkedin.com/company/diktamen-oy?tr...   \n","751        Kesko - K-Group  https://fi.linkedin.com/company/kesko?trk=publ...   \n","752          Visma Finland  https://fi.linkedin.com/company/visma-finland?...   \n","753              Digia Plc  https://fi.linkedin.com/company/digia_5119?trk...   \n","754              poolia.it  https://fi.linkedin.com/company/poolia-it-finl...   \n","\n","    contractType                                        description  \\\n","0      Full-time  Axopar, the adventure company, is one of the f...   \n","1      Full-time  Job Description\\n\\nAbout Merchant Group at Wol...   \n","2      Full-time  Starship Technologies is revolutionizing deliv...   \n","3      Full-time  This role required candidate to permanently re...   \n","4      Full-time  Job Description\\n\\nAbout the Consumer Group\\n\\...   \n","..           ...                                                ...   \n","750    Full-time  Type of employment contract: permanent / full-...   \n","751    Full-time  We are strengthening K group's software develo...   \n","752    Full-time  Come and build the future with Fikuro!\\n\\n\\n\\n...   \n","753    Full-time  We are looking for an experienced and skilled ...   \n","754     Contract  Hello, IT freelancer or entrepreneur - we are ...   \n","\n","      experienceLevel                                             jobUrl  ...  \\\n","0    Mid-Senior level  https://fi.linkedin.com/jobs/view/service-engi...  ...   \n","1    Mid-Senior level  https://fi.linkedin.com/jobs/view/data-scienti...  ...   \n","2    Mid-Senior level  https://fi.linkedin.com/jobs/view/senior-data-...  ...   \n","3    Mid-Senior level  https://fi.linkedin.com/jobs/view/global-oil-g...  ...   \n","4    Mid-Senior level  https://fi.linkedin.com/jobs/view/data-scienti...  ...   \n","..                ...                                                ...  ...   \n","750  Mid-Senior level  https://fi.linkedin.com/jobs/view/lead-mobile-...  ...   \n","751  Mid-Senior level  https://fi.linkedin.com/jobs/view/senior-full-...  ...   \n","752  Mid-Senior level  https://fi.linkedin.com/jobs/view/senior-front...  ...   \n","753  Mid-Senior level  https://fi.linkedin.com/jobs/view/devops-engin...  ...   \n","754  Mid-Senior level  https://fi.linkedin.com/jobs/view/it-arkkiteht...  ...   \n","\n","                                              workType work_arrangement  \\\n","0    Customer Service, Engineering, and Quality Ass...          On-site   \n","1                                              Analyst          On-site   \n","2                               Information Technology          On-site   \n","3                               Information Technology          On-site   \n","4                                              Analyst          On-site   \n","..                                                 ...              ...   \n","750                             Information Technology           Hybrid   \n","751             Engineering and Information Technology           Hybrid   \n","752             Engineering and Information Technology           Hybrid   \n","753             Engineering and Information Technology           Hybrid   \n","754              Information Technology and Consulting           Hybrid   \n","\n","    benefits            id language_posted                required skills  \\\n","0        NaN           NaN              en  C, Excel, Access, Power BI, R   \n","1        NaN           NaN              en                   R, C, Python   \n","2        NaN           NaN              en            R, C, Excel, Python   \n","3        NaN           NaN              en       R, C, Tensorflow, Python   \n","4        NaN           NaN              en                   R, C, Python   \n","..       ...           ...             ...                            ...   \n","750      NaN  4.030774e+09              fi              R, C, Excel, Java   \n","751      NaN  4.035517e+09              fi                    R, C, Excel   \n","752      NaN  4.038898e+09              fi              R, C, Excel, Java   \n","753      NaN  4.045803e+09              fi             R, C, Excel, Scala   \n","754      NaN  4.044930e+09              fi              R, C, Excel, Java   \n","\n","                                Data Base Applications  \\\n","0                                        Data Analysis   \n","1                SQL, Data Science, Data Visualization   \n","2                 SQL, Databricks, Data Transformation   \n","3    Data Science, Data Analysis, Data Quality, Dat...   \n","4    Data Engineering, Data Science, Data Analytics...   \n","..                                                 ...   \n","750                                            Unknown   \n","751                        Databricks, Data Processing   \n","752                                            Unknown   \n","753                                            Unknown   \n","754  Relational Database, Data Protection, Data Sec...   \n","\n","                         Data Engineering Cloud Computing  \\\n","0                                 Unknown         Unknown   \n","1                      Data Visualization             RDS   \n","2         Databricks, Data Transformation             RDS   \n","3                            Data Quality         Unknown   \n","4    Data Engineering, Data Visualization             RDS   \n","..                                    ...             ...   \n","750                               Unknown         Unknown   \n","751                 Data Flow, Databricks         Unknown   \n","752                               Unknown         Unknown   \n","753                               Unknown         Unknown   \n","754   Data Security, Relational Databases     RDS, Docker   \n","\n","                                           Soft Skills  \n","0    attention to detail, communication, adaptabili...  \n","1                                              Unknown  \n","2                                              Unknown  \n","3                                              Unknown  \n","4                                              Unknown  \n","..                                                 ...  \n","750                                        flexibility  \n","751  collaboration, teamwork, communication, creati...  \n","752                                            Unknown  \n","753                         communication, flexibility  \n","754                                      communication  \n","\n","[755 rows x 28 columns]\n","0      57\n","1      46\n","2      72\n","3      24\n","4      42\n","       ..\n","750    29\n","751    24\n","752    25\n","753    24\n","754    24\n","Name: applicationsCount, Length: 755, dtype: object\n","0      51-75\n","1      26-50\n","2      51-75\n","3       0-25\n","4      26-50\n","       ...  \n","750    26-50\n","751     0-25\n","752     0-25\n","753     0-25\n","754     0-25\n","Name: applicationsCount_interval, Length: 755, dtype: object\n","Processed applicationsCount and created new column applicationsCount_interval\n","0           Startup\n","1           Startup\n","2           Startup\n","3       Large-Scale\n","4           Startup\n","           ...     \n","750         Unknown\n","751     Large-Scale\n","752    Medium-Sized\n","753    Medium-Sized\n","754    Medium-Sized\n","Name: company_level, Length: 755, dtype: object\n","Processed description and created new column company_level\n","Processed postedTime and created new column job_displayedTime\n","Processed publishedAt and created new column published_month\n","0                                              [Service]\n","1                            [Data, Analytics, Merchant]\n","2                                         [Senior, Data]\n","3      [Global, Oil, Gas, Senior, Business, Systems, ...\n","4                            [Data, Analytics, Consumer]\n","                             ...                        \n","750                         [Lead, Mobile, Android, iOS]\n","751    [Senior, Full, Stack, grocery, development, te...\n","752                                   [Senior, Frontend]\n","753                                      [DevOps, Azure]\n","754                                [IT, s, Fullstack, s]\n","Name: job_title_desc_list, Length: 755, dtype: object\n","Processed title and created new column job_title_desc_list\n","0      [Customer Service, Engineering, Quality Assura...\n","1                                              [Analyst]\n","2                               [Information Technology]\n","3                               [Information Technology]\n","4                                              [Analyst]\n","                             ...                        \n","750                             [Information Technology]\n","751                [Engineering, Information Technology]\n","752                [Engineering, Information Technology]\n","753                [Engineering, Information Technology]\n","754                 [Information Technology, Consulting]\n","Name: worktype_desc, Length: 755, dtype: object\n","Processed workType and created new column worktype_desc\n","    applicationsCount applyType  \\\n","0                  57  EXTERNAL   \n","1                  46  EXTERNAL   \n","2                  72  EXTERNAL   \n","3                  24  EXTERNAL   \n","4                  42  EXTERNAL   \n","..                ...       ...   \n","748                34  EXTERNAL   \n","749                24  EXTERNAL   \n","750                29  EXTERNAL   \n","751                24  EXTERNAL   \n","752                25  EXTERNAL   \n","\n","                                              applyUrl   companyId  \\\n","0    https://rekry.biisoni.fi/en-GB/jobs/4901632-se...   2936094.0   \n","1    https://wolt.com/en/jobs/posting/7440000089556...   3995846.0   \n","2    https://starship.teamtailor.com/jobs/4933132-s...   6646555.0   \n","3    https://MatchaTalent.oorwin.com/careers/index....  14655911.0   \n","4    https://wolt.com/en/jobs/posting/7440000128565...   3995846.0   \n","..                                                 ...         ...   \n","748  https://apply.valagroup.com/fi/jobs/1531189-te...    892292.0   \n","749  https://careers.granlund.fi/jobs/5005126-senio...     75478.0   \n","750  https://atalent.fi/open-position/diktamen-lead...   2845630.0   \n","751  https://emp.jobylon.com/jobs/253891-k-ryhma-se...      7995.0   \n","752  https://ats.talentadore.com/apply/senior-front...  76114935.0   \n","\n","               companyName                                         companyUrl  \\\n","0             Axopar Boats  https://fi.linkedin.com/company/axopar-boats?t...   \n","1                     Wolt  https://fi.linkedin.com/company/wolt-oy?trk=pu...   \n","2    Starship Technologies  https://www.linkedin.com/company/starshiptechn...   \n","3             MatchaTalent  https://id.linkedin.com/company/matchatalent?t...   \n","4                     Wolt  https://fi.linkedin.com/company/wolt-oy?trk=pu...   \n","..                     ...                                                ...   \n","748                   OATH  https://fi.linkedin.com/company/vala-group-oy?...   \n","749               Granlund  https://fi.linkedin.com/company/granlund?trk=p...   \n","750              Dictation  https://fi.linkedin.com/company/diktamen-oy?tr...   \n","751        Kesko - K-Group  https://fi.linkedin.com/company/kesko?trk=publ...   \n","752          Visma Finland  https://fi.linkedin.com/company/visma-finland?...   \n","\n","    contractType                                        description  \\\n","0      Full-time  Axopar, the adventure company, is one of the f...   \n","1      Full-time  Job Description\\n\\nAbout Merchant Group at Wol...   \n","2      Full-time  Starship Technologies is revolutionizing deliv...   \n","3      Full-time  This role required candidate to permanently re...   \n","4      Full-time  Job Description\\n\\nAbout the Consumer Group\\n\\...   \n","..           ...                                                ...   \n","748    Full-time  Have you come across job postings where the li...   \n","749    Full-time  Do you want to influence the development of yo...   \n","750    Full-time  Type of employment contract: permanent / full-...   \n","751    Full-time  We are strengthening K group's software develo...   \n","752    Full-time  Come and build the future with Fikuro!\\n\\n\\n\\n...   \n","\n","      experienceLevel                                             jobUrl  ...  \\\n","0    Mid-Senior level  https://fi.linkedin.com/jobs/view/service-engi...  ...   \n","1    Mid-Senior level  https://fi.linkedin.com/jobs/view/data-scienti...  ...   \n","2    Mid-Senior level  https://fi.linkedin.com/jobs/view/senior-data-...  ...   \n","3    Mid-Senior level  https://fi.linkedin.com/jobs/view/global-oil-g...  ...   \n","4    Mid-Senior level  https://fi.linkedin.com/jobs/view/data-scienti...  ...   \n","..                ...                                                ...  ...   \n","748  Mid-Senior level  https://fi.linkedin.com/jobs/view/test-automat...  ...   \n","749  Mid-Senior level  https://fi.linkedin.com/jobs/view/senior-softw...  ...   \n","750  Mid-Senior level  https://fi.linkedin.com/jobs/view/lead-mobile-...  ...   \n","751  Mid-Senior level  https://fi.linkedin.com/jobs/view/senior-full-...  ...   \n","752  Mid-Senior level  https://fi.linkedin.com/jobs/view/senior-front...  ...   \n","\n","    company_level location_1        location_2 displayed_weeks  \\\n","0         Startup   Helsinki  Uusimaa, Finland        2.000000   \n","1         Startup   Helsinki  Uusimaa, Finland        2.000000   \n","2         Startup   Helsinki  Uusimaa, Finland        1.000000   \n","3     Large-Scale   Helsinki  Uusimaa, Finland        2.000000   \n","4         Startup   Helsinki  Uusimaa, Finland        0.142857   \n","..            ...        ...               ...             ...   \n","748   Large-Scale   Helsinki  Uusimaa, Finland        1.000000   \n","749   Large-Scale   Helsinki  Uusimaa, Finland        1.000000   \n","750       Unknown   Helsinki  Uusimaa, Finland        3.000000   \n","751   Large-Scale   Helsinki  Uusimaa, Finland        2.000000   \n","752  Medium-Sized   Helsinki  Uusimaa, Finland        1.000000   \n","\n","    job_displayedTime published_month general_job_category  \\\n","0                  3w              08           [Engineer]   \n","1                  3w              08          [Scientist]   \n","2                  2w              09            [Analyst]   \n","3                  3w              08            [Analyst]   \n","4                  1w              09          [Scientist]   \n","..                ...             ...                  ...   \n","748                2w              10           [Engineer]   \n","749                2w              10          [Developer]   \n","750                4w              09          [Developer]   \n","751                3w              09          [Developer]   \n","752                2w              10          [Developer]   \n","\n","                                        job_title_desc  \\\n","0                                              Service   \n","1                              Data Analytics Merchant   \n","2                                          Senior Data   \n","3    Global Oil Gas Senior Business Systems Data Sc...   \n","4                              Data Analytics Consumer   \n","..                                                 ...   \n","748                                    Test Automation   \n","749                           Senior Software Helsinki   \n","750                            Lead Mobile Android iOS   \n","751  Senior Full Stack grocery development team K G...   \n","752                                    Senior Frontend   \n","\n","                                   job_title_desc_list  \\\n","0                                            [Service]   \n","1                          [Data, Analytics, Merchant]   \n","2                                       [Senior, Data]   \n","3    [Global, Oil, Gas, Senior, Business, Systems, ...   \n","4                          [Data, Analytics, Consumer]   \n","..                                                 ...   \n","748                                 [Test, Automation]   \n","749                       [Senior, Software, Helsinki]   \n","750                       [Lead, Mobile, Android, iOS]   \n","751  [Senior, Full, Stack, grocery, development, te...   \n","752                                 [Senior, Frontend]   \n","\n","                                         worktype_desc  \n","0    [Customer Service, Engineering, Quality Assura...  \n","1                                            [Analyst]  \n","2                             [Information Technology]  \n","3                             [Information Technology]  \n","4                                            [Analyst]  \n","..                                                 ...  \n","748               [Information Technology, Consulting]  \n","749              [Engineering, Information Technology]  \n","750                           [Information Technology]  \n","751              [Engineering, Information Technology]  \n","752              [Engineering, Information Technology]  \n","\n","[561 rows x 39 columns]\n","required skills           0\n","Data Base Applications    0\n","Data Engineering          0\n","Cloud Computing           0\n","Soft Skills               0\n","dtype: int64\n","{'required skills': 0, 'Data Base Applications': 279, 'Data Engineering': 372, 'Cloud Computing': 171, 'Soft Skills': 138}\n"]}]},{"cell_type":"code","source":[" ### READ RAW FILES #########################\n","import pandas as pd\n","import os\n","\n","def read_files(folder_paths):\n","    \"\"\"\n","    Reads all CSV files from the specified folder paths and combines them into a single DataFrame.\n","\n","    Args:\n","        folder_paths (list): A list of folder paths containing CSV files.\n","\n","    Returns:\n","        DataFrame: A combined DataFrame containing all data from the CSV files.\n","    \"\"\"\n","    dataframes = []  # List to store individual DataFrames\n","\n","    # Loop through each folder\n","    for folder_path in folder_paths:\n","        # Get a list of all CSV files in the folder\n","        csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n","        print(f\"CSV files found in {folder_path}: {csv_files}\")  # Log found CSV files\n","\n","        # Loop through the list of CSV files and append each DataFrame to the list\n","        for file in csv_files:\n","            df = pd.read_csv(os.path.join(folder_path, file))\n","            if df.empty:\n","                print(f\"Warning: {file} is empty.\")\n","            else:\n","                dataframes.append(df)\n","                print(f\"Loaded DataFrame from {file}: {df.shape[0]} rows\")  # Print the number of rows loaded\n","\n","    # Check if there are DataFrames to concatenate\n","    if not dataframes:\n","        print(\"No DataFrames to concatenate. Please check your CSV files.\")\n","        return pd.DataFrame()  # Return an empty DataFrame if none found\n","\n","    # Concatenate all DataFrames into one\n","    combined_df = pd.concat(dataframes, ignore_index=True)\n","\n","    return combined_df\n"],"metadata":{"id":"m2ZMXwt-sTGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############# REMOVE DUPLICATES ##############################\n","\n","import pandas as pd\n","\n","def remove_duplicates(input_file, output_file):\n","    \"\"\"\n","    Removes duplicate rows from a CSV file and saves the cleaned data.\n","\n","    Args:\n","        input_file (str): Path to the input CSV file.\n","        output_file (str): Path to save the cleaned CSV file.\n","    \"\"\"\n","    # Load the combined CSV file\n","    df = pd.read_csv(input_file)\n","\n","    # Count the total number of records before removing duplicates\n","    initial_count = len(df)\n","\n","    # Strip leading/trailing spaces in all string columns\n","    df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n","\n","    # Remove identical (duplicate) rows\n","    df_cleaned = df.drop_duplicates()\n","\n","    # Count the total number of records after removing duplicates\n","    final_count = len(df_cleaned)\n","\n","    # Save the cleaned DataFrame to a new CSV file\n","    df_cleaned.to_csv(output_file, index=False)\n","\n","    # Print the counts\n","    print(f\"Number of records before removing duplicates: {initial_count}\")\n","    print(f\"Number of records after removing duplicates: {final_count}\")\n","    print(f\"Number of duplicate records removed: {initial_count - final_count}\")\n","    return df_cleaned"],"metadata":{"id":"8ZoTyx9etYf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install langdetect"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oq_TUBONRIuz","executionInfo":{"status":"ok","timestamp":1728815472742,"user_tz":-180,"elapsed":5633,"user":{"displayName":"Dilusha Senarathna","userId":"00628219531426179374"}},"outputId":"157e5f04-22c2-4dde-8076-17926d34dfe8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=3185910b7ef811d31f53722bf3830c56ab7fe3bffcd80a7f932037e035c84d1e\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}]},{"cell_type":"code","source":["pip install googletrans"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3Gt2zGtLtJK","executionInfo":{"status":"ok","timestamp":1728815486148,"user_tz":-180,"elapsed":13419,"user":{"displayName":"Dilusha Senarathna","userId":"00628219531426179374"}},"outputId":"34cf8fb9-d323-4a97-d8cc-109df3887948"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting googletrans\n","  Downloading googletrans-3.0.0.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting httpx==0.13.3 (from googletrans)\n","  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.8.30)\n","Collecting hstspreload (from httpx==0.13.3->googletrans)\n","  Downloading hstspreload-2024.10.1-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.1)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n","Collecting idna==2.* (from httpx==0.13.3->googletrans)\n","  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n","Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n","  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n","Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n","Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n","Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n","Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Downloading hstspreload-2024.10.1-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15718 sha256=d6bb0cd5aeedf5ed775a7f74b383f369a78fd4580cf6d23422cdc387f7fb1eee\n","  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.10\n","    Uninstalling idna-3.10:\n","      Successfully uninstalled idna-3.10\n","Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.10.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"]}]},{"cell_type":"code","source":[" ## STEP 3 : LANGUAGE TRANSLATION ###############\n","import pandas as pd\n","from langdetect import detect\n","from langdetect.lang_detect_exception import LangDetectException\n","\n","def add_language_column_and_filter(csv_file):\n","    # Read the CSV file\n","    df = pd.read_csv(csv_file)\n","\n","    # Function to detect language of the description\n","    def detect_language(text):\n","        try:\n","            return detect(text)\n","        except LangDetectException:\n","            return \"unknown\"\n","\n","    # Apply the function to the 'description' column and create a new 'language posted' column\n","    df['language_posted'] = df['description'].apply(detect_language)\n","\n","    # Extract records posted in Finnish (language code: 'fi')\n","    finnish_records = df[df['language_posted'] == 'fi']\n","\n","    # Save Finnish records to a new CSV file\n","    finnish_csv_file = csv_file.replace('.csv', '_finnish.csv')\n","    finnish_records.to_csv(finnish_csv_file, index=False)\n","\n","    # Drop Finnish records from the original DataFrame\n","    df_without_finnish = df[df['language_posted'] != 'fi']\n","\n","    # Save the updated DataFrame without Finnish records to a new CSV file\n","    updated_csv_file = csv_file.replace('.csv', '_without_finnish.csv')\n","    df_without_finnish.to_csv(updated_csv_file, index=False)\n","\n","    # Print the updated CSV headings\n","    print(\"Updated CSV Headings (without Finnish records):\")\n","    print(df_without_finnish.columns.tolist())\n","\n","    # Print the number of records in both CSV files\n","    print(f\"Number of records in the Finnish records CSV file: {len(finnish_records)}\")\n","    print(f\"Number of records in the updated CSV file (without Finnish records): {len(df_without_finnish)}\")\n","\n","    return finnish_csv_file, updated_csv_file\n","\n","\n","import pandas as pd\n","\n","def combine_csv_files(without_finnish_csv, translated_finnish_csv):\n","    # Read the CSV files\n","    df_without_finnish = pd.read_csv(without_finnish_csv)\n","    df_translated_finnish = pd.read_csv(translated_finnish_csv)\n","\n","    # Combine the DataFrames\n","    combined_df = pd.concat([df_without_finnish, df_translated_finnish], ignore_index=True)\n","\n","    # Save the combined DataFrame to a new CSV file\n","    combined_csv_file_path = '/content/drive/MyDrive/Intro_DS/MiniProject/outputs/combined_csv_file.csv'\n","    combined_df.to_csv(combined_csv_file_path, index=False)\n","\n","    # Print the number of records in the combined DataFrame\n","    print(f\"Number of records in the Combined CSV file: {len(combined_df)}\")\n","\n","    print(f\"Combined CSV saved as {combined_csv_file_path}\")\n","    return combined_csv_file_path\n","\n"],"metadata":{"id":"meSNNraoyj3q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### STEP 4 : FEATURE ENGINEERING PART\n","\n","# DESCRIPTION COLUMN\n","import pandas as pd\n","\n","def read_keywords(file_path):\n","    \"\"\"\n","    Reads keyword categories from a txt file and returns a dictionary of lists.\n","    \"\"\"\n","    keywords = {}\n","    current_category = None\n","\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            line = line.strip()\n","            if line.startswith('[') and line.endswith(']'):\n","                current_category = line[1:-1]\n","                keywords[current_category] = []\n","            elif current_category:\n","                keywords[current_category] = [x.strip() for x in line.split(',')]\n","\n","    return keywords\n","\n","def extract_info(description, keywords):\n","    \"\"\"\n","    Extracts skills, databases, data engineering, cloud computing, and soft skills\n","    from the job description based on provided keywords.\n","    \"\"\"\n","    skills, databases, data_engineering, cloud_computing, soft_skills = set(), set(), set(), set(), set()\n","\n","    # Check for keywords in each category\n","    for skill in keywords.get('skills_keywords', []):\n","        if skill.lower() in description.lower():\n","            skills.add(skill)\n","\n","    for db in keywords.get('database_keywords', []):\n","        if db.lower() in description.lower():\n","            databases.add(db)\n","\n","    for eng in keywords.get('data_engineering_words', []):\n","        if eng.lower() in description.lower():\n","            data_engineering.add(eng)\n","\n","    for cloud in keywords.get('cloud_computing_words', []):\n","        if cloud.lower() in description.lower():\n","            cloud_computing.add(cloud)\n","\n","    for soft in keywords.get('soft_skills_word', []):\n","        if soft.lower() in description.lower():\n","            soft_skills.add(soft)\n","\n","    # Return a comma-separated list of found keywords\n","    return (\n","        ', '.join(skills) if skills else 'Unknown',\n","        ', '.join(databases) if databases else 'Unknown',\n","        ', '.join(data_engineering) if data_engineering else 'Unknown',\n","        ', '.join(cloud_computing) if cloud_computing else 'Unknown',\n","        ', '.join(soft_skills) if soft_skills else 'Unknown'\n","    )\n"],"metadata":{"id":"Zp4EYePgyqpQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Number of appliants column processing\n","# Function to extract the number from the string and subtract 1, or keep the number as is\n","\n","def replace_qualitative_value(value):\n","    if isinstance(value, str) and 'Be among the first' in value:\n","        # Extract the number using string split\n","        num = int(value.split()[4])  # \"Be among the first X applicants\" --> extract X\n","        return num - 1  # Subtract 1 from the number\n","    elif value.isdigit():  # Check if the value is already a number (as a string)\n","        return int(value)  # Convert it to integer\n","    else:\n","        return value  # Return the value as is if it's not a string or numeric"],"metadata":{"id":"EWhLLkn5sozE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Function to categorize job applicants counts into 25-interval ranges up to 200\n","def categorize_into_intervals(df, column_name):\n","    # Apply the categorization to the specified column\n","    df[column_name + '_interval'] = df[column_name].apply(lambda x: categorize_value(x))\n","    return df  # Return the updated DataFrame\n","\n","# Categorization logic for each value in the specified column\n","def categorize_value(value):\n","    try:\n","        num = int(value)\n","        if num <= 25:\n","            return '0-25'\n","        elif num <= 50:\n","            return '26-50'\n","        elif num <= 75:\n","            return '51-75'\n","        elif num <= 100:\n","            return '76-100'\n","        elif num <= 125:\n","            return '101-125'\n","        elif num <= 150:\n","            return '126-150'\n","        elif num <= 175:\n","            return '151-175'\n","        elif num <= 200:\n","            return '176-200'\n","        else:\n","            return 'Over 200'  # For values above 200\n","    except ValueError:\n","        return value  # In case of non-numeric or unexpected value\n"],"metadata":{"id":"pLuMFLUjvsa2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to classify company level based on job description\n","# Function to read keywords from a file\n","def read_keywords_from_file(file_path):\n","    keywords = {}\n","    with open(file_path, 'r') as file:\n","        current_category = None\n","        for line in file:\n","            line = line.strip()\n","            if line.startswith(\"[\"):  # Check if the line is a category\n","                current_category = line[1:-1]  # Remove brackets\n","                keywords[current_category] = []\n","            elif current_category is not None and line:\n","                # Add keywords to the current category\n","                keywords[current_category].extend(line.split(\", \"))\n","    return keywords\n","\n","# Updated classify function\n","def classify_company_level(description, keywords):\n","    description = description.lower()\n","\n","    # Check for startup keywords\n","    if any(keyword in description for keyword in keywords['company_keywords_startup']):\n","        return 'Startup'\n","\n","    # Check for medium-sized company keywords\n","    elif any(keyword in description for keyword in keywords['company_keywords_medium']):\n","        return 'Medium-Sized'\n","\n","    # Check for large-scale company keywords\n","    elif any(keyword in description for keyword in keywords['company_keywords_large']):\n","        return 'Large-Scale'\n","\n","    # If no match, return unknown\n","    return 'Unknown'\n"],"metadata":{"id":"VjwKCrHV0eVX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# derived variables : 'job_displayedTime'  in weeks\n","import re\n","\n","# Remove the word 'ago' and convert to a standard format\n","def convert_to_weeks(time_str):\n","    if pd.isnull(time_str):\n","        return None\n","\n","    # Remove 'ago' and handle the time string\n","    time_str = time_str.replace(' ago', '')\n","\n","    # Extract numbers and time units\n","    match = re.match(r'(\\d+)\\s*(day|week|month|hour|minute)s?', time_str)\n","    if match:\n","        value, unit = match.groups()\n","        value = int(value)\n","\n","        # Convert to weeks\n","        if unit in ['day', 'days']:\n","            return value / 7\n","        elif unit in ['week', 'weeks']:\n","            return value\n","        elif unit in ['month', 'months']:\n","            return value * 4  # Approximate a month as 4 weeks\n","        elif unit in ['hour', 'hours']:\n","            return value / 168  # 168 hours in a week\n","        elif unit in ['minute', 'minutes']:\n","            return value / 10080  # 10080 minutes in a week\n","\n","    return None\n","\n","\n","# Define a function to categorize the weeks into the required format\n","def categorize_weeks(weeks):\n","    if weeks is None:\n","        return None\n","    elif weeks < 1:\n","        return '1w'\n","    elif 1 <= weeks < 2:\n","        return '2w'\n","    elif 2 <= weeks < 3:\n","        return '3w'\n","    elif 3 <= weeks < 4:\n","        return '4w'\n","    elif 4 <= weeks < 8:\n","        return '4w+'\n","    elif 8 <= weeks < 12:\n","        return '8w+'\n","    else:\n","        return '12w+'"],"metadata":{"id":"wVHYJQlWcT2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to extract tokens and remove them from the remaining text\n","def extract_tokens_and_text(title):\n","    # Updated list of tokens to check for, excluding 'trainee'\n","    tokens = ['Scientist','Engineer','Thesis worker', 'Researcher', 'Data Steward', 'Consultant',\n","          'Analyst', 'manager', 'modeler', 'Specialist', 'Developer',\n","          'Officer', 'Product Owner', 'Architect', 'Ethicist','Internship']\n","    # List of stop words to remove from remaining text\n","    stop_words = {'in', 'for', 'and', 'of', 'the', 'to', 'with', 'a', 'on', 'by'}\n","\n","    title_lower = title.lower()\n","    matched_tokens = [token for token in tokens if token.lower() in title_lower]\n","\n","    # Remove matched tokens from the title to create the remaining text\n","    for token in matched_tokens:\n","        title = re.sub(re.escape(token), '', title, flags=re.IGNORECASE).strip()\n","\n","    # Filter out stop words from the remaining text\n","    remaining_words = [word for word in re.findall(r'\\b\\w+\\b', title) if word.lower() not in stop_words]\n","    remaining_text = ' '.join(remaining_words)\n","\n","    return matched_tokens, remaining_text"],"metadata":{"id":"-q3WfWMfeu48"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to split text by commas and handle stop words\n","def split_and_filter(text):\n","    stop_words = {'in', 'for', 'and', 'the', 'of', 'to', 'with', 'a', 'on', 'by'}\n","    if isinstance(text, str):  # Ensure text is a string\n","        # Split by comma and 'and'\n","        parts = text.split(',')\n","        split_words = []\n","        for part in parts:\n","            # Split by 'and'\n","            sub_parts = part.split('and')\n","            for sub_part in sub_parts:\n","                # Remove stop words and leading/trailing spaces\n","                cleaned_part = ' '.join(word.strip() for word in sub_part.split() if word.lower() not in stop_words)\n","                if cleaned_part:\n","                    split_words.append(cleaned_part)\n","        return split_words\n","    return []  # Return an empty list if text is not a string\n"],"metadata":{"id":"yxfcajJogc7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","def data_profiling(df, duplicates_path='duplicates.csv', summary_path='before_cleaning.txt'):\n","    \"\"\"\n","    Perform data profiling on the given DataFrame, save duplicates, and drop them from the DataFrame.\n","\n","    Parameters:\n","        df (pd.DataFrame): The DataFrame to analyze.\n","        col1 (str): The first column to check for duplicates.\n","        col2 (str): The second column to check for duplicates.\n","        duplicates_path (str): The path to save duplicates for investigation.\n","        summary_path (str): The path to save the summary of metrics.\n","\n","    Returns:\n","        pd.DataFrame: A DataFrame containing the cleaned dataset with non-null company URLs.\n","    \"\"\"\n","\n","    profile_summary = []\n","\n","    # Identify duplicates based on the specified columns\n","    duplicates = df[df.duplicated(subset=['title', 'companyName'], keep=False)]  # Keep all duplicates\n","    duplicate_count = duplicates.shape[0]\n","\n","    # Save duplicates to CSV for investigation\n","    duplicates.to_csv(duplicates_path, index=False)\n","\n","    # Count null values and store detailed info\n","    null_counts = df.isnull().sum()\n","    for col, count in null_counts[null_counts > 0].items():\n","        profile_summary.append([f'Null count of {col}', count])  # Append each column's null count\n","\n","    # Count missing values (NaN and None)\n","    missing_counts = df.isna().sum()\n","    for col, count in missing_counts[missing_counts > 0].items():\n","        profile_summary.append([f'Missing count of {col}', count])  # Append each column's missing count\n","\n","    # Summary of dataset shape\n","    profile_summary.append(['Shape', df.shape])  # Append shape as a tuple\n","    profile_summary.append(['Duplicate Count', duplicate_count])  # Append duplicate count\n","\n","    # Create a vertical summary DataFrame\n","    vertical_summary = pd.DataFrame(profile_summary, columns=['Metric', 'Value'])\n","\n","    # Save the summary to a text file\n","    with open(summary_path, 'w') as f:\n","        for index, row in vertical_summary.iterrows():\n","            f.write(f\"{row['Metric']}: {row['Value']}\\n\")\n","\n","    # Drop duplicates from the DataFrame\n","    df_cleaned = df.drop_duplicates(subset=['title', 'companyName'], keep='first')  # Keep the first occurrence\n","\n","    # Filter rows where 'companyUrl' is not null\n","    df_cleaned = df_cleaned[df_cleaned['companyUrl'].notnull()]\n","\n","    return df_cleaned\n","\n","# Example of using the function\n","# df = pd.read_csv('your_dataset.csv')  # Load your DataFrame\n","# cleaned_df = data_profiling(df, duplicates_path='duplicates.csv', summary_path='before_cleaning.txt')\n","# cleaned_df.to_csv('cleaned_dataset.csv', index=False)  # Save the cleaned DataFrame\n"],"metadata":{"id":"NfAY9Uw9gl1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Unknown value count check\n","import pandas as pd\n","\n","def count_unknown_values_in_csv(file_path, columns):\n","    \"\"\"\n","    Count the number of 'unknown' values in specified columns of a CSV file.\n","\n","    Parameters:\n","    - file_path (str): The path to the CSV file.\n","    - columns (list of str): The list of column names to search for 'unknown' values.\n","\n","    Returns:\n","    - dict: A dictionary with column names as keys and counts of 'unknown' as values.\n","    \"\"\"\n","    # Load the CSV file into a DataFrame\n","    df = pd.read_csv(file_path)\n","\n","    # Initialize dictionary to store counts\n","    unknown_counts = {}\n","\n","    # Iterate through specified columns and count 'unknown' values\n","    for column in columns:\n","        if column in df.columns:\n","            unknown_counts[column] = df[column].str.lower().eq('unknown').sum()\n","        else:\n","            unknown_counts[column] = 'Column not found'\n","\n","    return unknown_counts"],"metadata":{"id":"P-cWuRA_VRO8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vnEiD4odpwod","executionInfo":{"status":"ok","timestamp":1728814915272,"user_tz":-180,"elapsed":22973,"user":{"displayName":"Dilusha Senarathna","userId":"00628219531426179374"}},"outputId":"7b9b90c7-b8f1-49f1-8015-8336a6a745c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JBSImus4DXqX"},"execution_count":null,"outputs":[]}]}